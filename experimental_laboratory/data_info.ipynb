{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bf67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as tts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529ed738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook dir. Путь к этой тетради и разархивированным датасетам calls и CallsDataset\n",
    "dir_path = %pwd\n",
    "# сигналы false {dir_path+\"\\\\calls\\\\false\"}\n",
    "# сигналы true {dir_path+\"\\\\calls\\\\true\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22524ea",
   "metadata": {},
   "source": [
    "```python\n",
    "# Для создания таблиц все файлы из демо и рабочей директории раскинул по двум папкам calls\\false и calls\\true\n",
    "def dct():\n",
    "    count = 0\n",
    "    dct = {}\n",
    "    dct['id'] = []\n",
    "    dct['name'] = []\n",
    "    dct['duration'] = []\n",
    "    for file in os.listdir():\n",
    "        y, sr = librosa.load(file)\n",
    "        count += 1\n",
    "        dct['id'].append(count)\n",
    "        dct['name'].append(file)\n",
    "        dct['duration'].append(int(len(y)/sr))\n",
    "    return pd.DataFrame(dct)\n",
    "\n",
    "# path = dir_path\n",
    "\n",
    "#%cd {dir_path+\"\\\\calls\\\\false\"}\n",
    "# Сохраняем измененную таблицу в новый CSV-файл\n",
    "#dct().to_csv(path+'false.csv', index=False)\n",
    "\n",
    "#%cd {dir_path+\"\\\\calls\\\\true\"}\n",
    "# Сохраняем измененную таблицу в новый CSV-файл\n",
    "#dct().to_csv(path+'true.csv', index=False)\n",
    "\n",
    "%cd {dir_path}\n",
    "\n",
    "print('false.csv', 'duration', np.median(pd.read_csv('false.csv')['duration']))\n",
    "print('true.csv', 'duration', np.median(pd.read_csv('true.csv')['duration']))\n",
    "```\n",
    "\n",
    "```\n",
    "false.csv duration 4.0\n",
    "true.csv duration 36.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d80e3",
   "metadata": {},
   "source": [
    "```python\n",
    "# time split\n",
    "sec = 20\n",
    "segments_sec_label = f'segments_{sec}sec'\n",
    "print(segments_sec_label)\n",
    "\n",
    "# add col comment to data. Генератор списка аугментаций к сигналу в зависимости от его длины\n",
    "aug_gen = (lambda text='': 'augs-- gen' + text\n",
    "           + np.random.choice(['', ' white_noise', ' mx2y', \n",
    "                               ' mxy', ' low_hz', ' avg', \n",
    "                               ' slow_down', ' echo', ' relu', \n",
    "                               ' mn2y', ' mny', ' reverse', \n",
    "                               ' stft', ' noised_echo', ' high_hz', \n",
    "                               ' pink_noise']))\n",
    "comment = lambda x: aug_gen(\" repeat\") if x < sec else aug_gen()\n",
    "# Пример comment(len(y)/sr): augs-- gen white_noise mx2y avg noised_echo\n",
    "\n",
    "# add col original_path to data\n",
    "def original_path(label):\n",
    "    file_name = ['FalseSilent', 'FalseSounds', 'FalseSpeech', 'TrueSilent',\n",
    "                 'TrueSounds', 'TrueSpeech', 'false', 'true']\n",
    "    file_dir =  [\"CallsDataset\\\\Ложь_тишин\", # 'FalseSilent'\n",
    "                \"CallsDataset\\\\Ложь_посторонние_звуки\", # 'FalseSounds'\n",
    "                \"CallsDataset\\\\Ложь_разговоры\", # 'FalseSpeech'\n",
    "                \"CallsDataset\\\\Истина_тишина\", # 'TrueSilent'\n",
    "                \"CallsDataset\\\\Истина_посторонние_звуки\", # 'TrueSounds'\n",
    "                \"CallsDataset\\\\Истина_разговоры\", # 'TrueSpeech'\n",
    "                \"calls\\\\false\", # 'false'\n",
    "                \"calls\\\\true\"] # 'true'\n",
    "    file_dict = dict(zip(file_name, file_dir))\n",
    "    label = label.split(' ')[0]\n",
    "    return file_dict[label]\n",
    "\n",
    "# add col new_file_names to data\n",
    "def new_file_names(data):\n",
    "    cls = lambda x: 'true' if x==1 else 'false'\n",
    "    lst = []\n",
    "    data = data.reset_index(drop=True)\n",
    "    for i in range(len(data)):\n",
    "        row = data.loc[i, ['label', 'id', 'sample', 'cls']]\n",
    "        name = ' id-'.join(map(str,[row[0], row[1]]))+'.mp3'\n",
    "        path = f\"dataset\\\\{row[2]}\\\\{cls(row[3])}\\\\\" + name\n",
    "        lst.append(path)\n",
    "    data['path'] = lst\n",
    "    return data\n",
    "\n",
    "# repeat rows\n",
    "def add_rows(data, k=None):\n",
    "    def _add_rows(data, i, k):\n",
    "        if k == None:\n",
    "            k = data.loc[i,segments_sec_label]\n",
    "        if k > 1:\n",
    "            k -= 1\n",
    "            new_data = pd.concat([data[i:i+1]]*k, axis=0)\n",
    "            data = pd.concat([data, new_data])\n",
    "        return data\n",
    "    for i in range(len(data)):\n",
    "        data = _add_rows(data, i, k)\n",
    "    data = data.sort_values(['id', 'label', 'segment_number']).reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# add label, file_type, cls, sample,\n",
    "# add segments_sec, segment_number, start_sec, \n",
    "# add stop_sec, segment_type, origin_duration,\n",
    "# add comment, new_file_names\n",
    "# add rows\n",
    "# update id, duration\n",
    "# to data\n",
    "def upgrade_data(label):\n",
    "    data = pd.read_csv(label)\n",
    "    \n",
    "    # name --> label, file_type\n",
    "    def split_name(data, del_name=False):\n",
    "        data[[\"label\", \"file_type\"]] = data[\"name\"].str.split(\".\", n=1, expand=True)\n",
    "        if del_name: data.drop(\"name\", axis=1, inplace=True)\n",
    "        return data    \n",
    "    data = split_name(data, del_name=False)\n",
    "    \n",
    "    # original_path\n",
    "    data['original_path'] = data['label'].apply(lambda label: original_path(label)+'\\\\'+label+'.mp3')\n",
    "    \n",
    "    # cls\n",
    "    data = data.assign(cls=lambda x: 1 if label.split('.')[0]=='true' else 0)\n",
    "    \n",
    "    # sample\n",
    "    def sample(data):\n",
    "        s = ['val', 'val', 'test', 'train', 'train', 'train', 'train', 'train', 'train', 'train']\n",
    "        sample = []\n",
    "        count = 0\n",
    "        data = data.sort_values('duration', ascending=False)\n",
    "        for x in data['duration']:\n",
    "            sample.append(s[count])\n",
    "            if count < len(s)-1: count += 1\n",
    "            else: count = 0\n",
    "        data['sample'] = sample\n",
    "        return data.sort_values('id', ascending=True)\n",
    "    data = sample(data)\n",
    "    \n",
    "    # segments_sec, segment_number, start_sec, stop_sec, segment_type, duration, origin_duration\n",
    "    def segment_options(data):\n",
    "        # origin_duration\n",
    "        data['original_duration'] = data['duration']\n",
    "        # segments_sec with shift 50%\n",
    "        shift = 2\n",
    "        data[segments_sec_label] = data['original_duration'].apply(lambda x: int(x/sec)*shift)\n",
    "        def segment_time(data):\n",
    "            segments = np.unique(data[segments_sec_label]).item()\n",
    "            duration = np.unique(data['original_duration']).item()\n",
    "            s = int(sec/shift)\n",
    "            start = [x*s for x in range(segments)]\n",
    "            finish = [(x*s)+shift*s for x in range(segments-1)]+[duration]\n",
    "            return start, finish  \n",
    "        # segment_type\n",
    "        data = data.assign(segment_type=lambda x: 'full_audio')\n",
    "        # segment_number\n",
    "        data = data.assign(segment_number=lambda x: 0)\n",
    "        # start_sec, stop_sec\n",
    "        data = data.assign(start_sec=lambda x: 0).assign(stop_sec=lambda x: data['duration'])    \n",
    "        # repeat rows. id * segments_sec_label\n",
    "        data = add_rows(data)    \n",
    "        # set start_sec, stop_sec, segment_type for new_data.  data_0_1(original audio), new_data(fragments) --> data\n",
    "        data_0_1 = data.query(f'{segments_sec_label} in [0,1]')\n",
    "        data = data.query(f'{segments_sec_label} not in [0,1]').reset_index(drop=True)\n",
    "        new_data = data[0:0]\n",
    "        for i in np.unique(data['id']):\n",
    "            fragment = data.query(f'id == {i}').reset_index(drop=True)\n",
    "            fragment['segment_number'] = np.arange(len(fragment))\n",
    "            start, stop = segment_time(fragment)\n",
    "            fragment['start_sec'] = start\n",
    "            fragment['stop_sec'] = stop\n",
    "            fragment['segment_type'] = ['segment']*len(fragment)\n",
    "            new_data = pd.concat([new_data, fragment])\n",
    "        data = pd.concat([data_0_1, new_data]).reset_index(drop=True)   \n",
    "        # update duration\n",
    "        data['duration'] = data['stop_sec']-data['start_sec']\n",
    "        return  data\n",
    "    data = segment_options(data)\n",
    "    # update id\n",
    "    data = data.sort_values(['id','segment_number'])\n",
    "    data['id'] = np.arange(len(data))\n",
    "    columns_lst = ['label', 'cls', 'id', 'sample', 'original_duration', segments_sec_label, \n",
    "                   'segment_type', 'segment_number', 'duration', 'start_sec', 'stop_sec', 'original_path']\n",
    "    data = data[columns_lst]\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "# true, false*k --> all\n",
    "def result_data(false, true, false_k=4, reversion_shift=True):\n",
    "    # false --> false*k\n",
    "    false = add_rows(false, k=false_k)\n",
    "    # len(false) --> len(false)=len(true)\n",
    "    def balanced_true_class(false, true, k=1):\n",
    "        def query(sample):\n",
    "            s = true.copy()\n",
    "            s = s.query(f'sample == \"{sample}\"')\n",
    "            s = s.sort_values('duration', ascending=False)\n",
    "            s = s.reset_index(drop=True)\n",
    "            l = len(false.query(f'sample == \"{sample}\"'))*k\n",
    "            s = s[:l]\n",
    "            return s\n",
    "        return pd.concat([query(\"train\"),query(\"val\"),query(\"test\")]).reset_index(drop=True)\n",
    "    true = balanced_true_class(false, true, k=1)\n",
    "    # true + false --> all\n",
    "    data = pd.concat([true,false])\n",
    "    # all*3\n",
    "    data = pd.concat([data]*2)\n",
    "    # короткие сегменты портят статистику в тестовых выборках. \n",
    "    # пусть будут в трейне, больше пользы.\n",
    "    lst = data.query('sample == \"test\" and duration < 5').index\n",
    "    data.drop(index=lst, inplace=True)\n",
    "    \n",
    "    #######################\n",
    "    data = data.sort_values(['cls', 'id', 'label', 'segment_number']).reset_index(drop=True)\n",
    "    smpls = [\"train\", \"val\"]\n",
    "    \n",
    "    # reversion shift. отступ между стартами сегментов для выборок в smpls\n",
    "    if reversion_shift:\n",
    "        # индексы каждого 4 сегмента длинного сигнала. Создаем разницу между стартами сегментов 4 x 3 секунд\n",
    "        lst = np.arange(0, 559, 4)\n",
    "        # список лишних сегментов - все, кроме каждого 4 сегмента.\n",
    "        lst = data.query('sample in @smpls and segment_number not in @lst').index\n",
    "        # Удалим все строки по индексам кроме каждого 4 значения, изменив исходный датафрейм\n",
    "        data.drop(index=lst, inplace=True)\n",
    "\n",
    "    # comment\n",
    "    data['comment'] =data['duration'].apply(lambda x: comment(x))\n",
    "    \n",
    "    # update id\n",
    "    data = data.sort_values(['cls', 'id', 'label', 'segment_number']).reset_index(drop=True)\n",
    "    data['id'] = np.arange(len(data))\n",
    "    \n",
    "    # new_file_name\n",
    "    data = new_file_names(data)\n",
    "    return data\n",
    "\n",
    "%cd {dir_path}\n",
    "false = upgrade_data('false.csv').query('original_duration >= 15')\n",
    "true = upgrade_data('true.csv').query('original_duration >= 15')\n",
    "all_data = result_data(false, true, false_k=45, reversion_shift=True)\n",
    "all_data.to_csv('revshift_20-2.csv', index=False)\n",
    "\n",
    "# show result\n",
    "def items(data):\n",
    "    def items(data, cls, sample):\n",
    "        items = lambda data, cls, sample: data.query(f'cls == {cls} and sample == \"{sample}\"')['duration']\n",
    "        s = items(data, cls, sample)\n",
    "        # str: sample files count\n",
    "        result = sample + ': ' + str(len(s)) + ' files. '\n",
    "        # str: sample duration\n",
    "        result_2 = str(sum(s)) + ' sec. '\n",
    "        # str: Average duration for sample\n",
    "        result_3 = 'Average: ' + str(np.median(s)) + ' sec.'\n",
    "        return print(result + result_2 + result_3)\n",
    "    for cls in [0, 1]:\n",
    "        if cls==0: head = '\\nfalse '\n",
    "        else: head = '\\n\\ntrue '\n",
    "        print(head, len(data.query(f'cls == {cls}')))\n",
    "        for sample in [\"train\", \"val\", \"test\"]:\n",
    "            items(data, cls, sample)\n",
    "            \n",
    "items(all_data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9aa4c9",
   "metadata": {},
   "source": [
    "```\n",
    "segments_20sec\n",
    "C:\\Users\\dtata\\Downloads\\AI Machine Learning\\emergency_calls\\back_up\\dataset_revshift\n",
    "\n",
    "false  6840\n",
    "train: 3780 files. 72720 sec. Average: 20.0 sec.\n",
    "val: 1710 files. 33840 sec. Average: 20.0 sec.\n",
    "test: 1350 files. 25380 sec. Average: 20.0 sec.\n",
    "\n",
    "\n",
    "true  1660\n",
    "train: 918 files. 18178 sec. Average: 20.0 sec.\n",
    "val: 292 files. 5778 sec. Average: 20.0 sec.\n",
    "test: 450 files. 8882 sec. Average: 20.0 sec.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a0a2a",
   "metadata": {},
   "source": [
    "```python\n",
    "# Последний этап шлифовки. Балансировка числа аугментаций.\n",
    "# Нужно сократить false в train и val.\n",
    "all_data = pd.read_csv('revshift_20-2.csv')\n",
    " \n",
    "train_data_cls0 = all_data.query('sample == \"train\" and cls == 0 and comment != \"augs-- \"').reset_index(drop=True)\n",
    "train_data_cls0 = train_data_cls0.loc[np.arange(0,len(train_data_cls0), 4)]\n",
    "train_data_cls1 = all_data.query('sample == \"train\" and cls == 1')\n",
    "\n",
    "val_data_cls0 = all_data.query('sample == \"val\" and cls == 0 and comment != \"augs-- \"').reset_index(drop=True)\n",
    "val_data_cls0 = val_data_cls0.loc[np.arange(0,len(val_data_cls0), 6)]\n",
    "val_data_cls1 = all_data.query('sample == \"val\" and cls == 1')\n",
    "\n",
    "test_data = all_data.query('sample == \"test\"')\n",
    "all_data = pd.concat([train_data_cls0, train_data_cls1, val_data_cls0, val_data_cls1, test_data])\n",
    "\n",
    "# новый признак - count. Считает число повторений одного сегмента в таблице\n",
    "# count = оригинальный_сегмент + аугментированный_сегмент * x\n",
    "cols = ['label', 'segment_number']\n",
    "data_vc = all_data.value_counts(cols).reset_index()\n",
    "all_data = pd.merge(all_data, data_vc, on=cols)\n",
    "\n",
    "# фильтруем тестововую выборку до уникальных значений более 15 секунд. Удаляем аугментации\n",
    "test_data = pd.read_csv('revshift_20-2.csv')\n",
    "test_data = test_data.sort_values(['label', 'segment_number'])\n",
    "test_data = test_data.query('sample == \"test\"').drop_duplicates(['label'])\n",
    "test_data = test_data.assign(comment='augs-- ').assign(count=1)\n",
    "\n",
    "all_data = pd.concat([all_data.query('sample not in \"test\"'), \n",
    "                      test_data]).query('duration >= 15').reset_index(drop=True)\n",
    "all_data['original_dir_cls'] = all_data['original_path'].apply(lambda x: x.split(\"\\\\\")[1])\n",
    "items(all_data)\n",
    "all_data.to_csv('revshift_20-2.csv', index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fac01",
   "metadata": {},
   "source": [
    "```\n",
    "false  1236\n",
    "train: 945 files. 18184 sec. Average: 20.0 sec.\n",
    "val: 285 files. 5640 sec. Average: 20.0 sec.\n",
    "test: 6 files. 118 sec. Average: 20.0 sec.\n",
    "\n",
    "\n",
    "true  1251\n",
    "train: 918 files. 18178 sec. Average: 20.0 sec.\n",
    "val: 292 files. 5778 sec. Average: 20.0 sec.\n",
    "test: 41 files. 804 sec. Average: 20.0 sec.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
